# :construction: Robot Survives Zombies :construction:
Project for CPSC 524 "Intelligent Robotics"

## TL;DR
A robot that navigates a terrain autonomously to avoid zombies and pursue fuel in Webots simulation.

## Usage
First install [Webots](https://cyberbotics.com/) if you haven't done so. Then choose a world. Then start the simulation and the robot will start running!

## Design Decisions
Our overall strategy for survival is going in the most promising direction at any time. In terms of sensor choices, we use 4 cameras--front, back, left, and right--such that we get a corresponding image in each direction. We parse each camera image to obtain a value associated with that image. The robot then goes in the direction with the highest value image. At a high level, this architecture is DAMN, where each behavior (go forward, go back, turn left, and turn right) votes, and the vote is weighted more heavily when the associated image's value is higher. The robot chooses the behavior with the most weighted vote. We recognize that the 4 images don't cover the whole 360-degree view, but we also observe that the "blind spots" won't cause significant trouble because the robot tends to keep moving instead of stopping (see below), and the likelihood of zombies staying in the blind spot is low, especially with closer zombies.


To obtain a value associated with each image, we employ two design principles. We 1) leverage a command fusion model for high level navigation and 2) complement it with a local subsumption model for zombie/wall avoidance and berry pursuit. For each of the four directions, we determine a value for that image through a linear combination of the objects present in that camera image. For each type of berries, zombies, walls, stump etc, we give corresponding weights that captures their benefit to the robot. Specifically, the value of an object in an image is roughly proportional to the fourth power of the difference between its bottom coordinate and the center, and also proportional to the pixel distance of its center to The more positive the aggregate value, the more beneficial moving in that direction would be. However, even after extensive weight tuning, there are situations in which the robot doesnâ€™t avoid imminent threats. For instance, an image containing multiple berries with a close by zombie might result in a weighted sum that is comparable with an image containing no objects of interest (i.e. just a tree in the background). Command fusion on its own is not aware of the correct action--evading the zombie. Therefore, we implemented a local subsumption architecture so that the robot recognizes the presence of a zombie and escalates by overriding the current movement with a command to evade the zombie. Specifically, if a zombie is detected to be close by, or a berry is detected to lie in the center of the camera image, or a wall is detected to block the way the robot is moving, then we make sure the robot is behaving in the desired way by suppressing the output of the command fusion architecture. The local model always subsumes the high level model. 


To address additional changes in the world, the robot keeps track of the berries it has eaten and these berries' associated effect in a table. Based on the empirical counts of each type and its effects, the robot updates weights for berries such that it prioritizes berries whose main effect (determined based on effects with more empirical counts) is adding energy / health especially when its energy / health is low, and avoids the type of berry whose main effect is losing energy. Additionally, the robot uses its arm to push berries off tree stumps when it's close to a stump; this behavior has low priority, so it's only done when the robot has energy and health > 80, and is not being chased by zombies (when local best direction returns -1). To address different colors of zombies, we assigned different negative weights: -2.5, -4, -3, -2 for AquaZombie, PurpleZombie, BlueZombie, GreenZombie, to reflect their danger level and obtained these relatively well-performing values after some experimentation. We chose not to address the additional of trees, because 1) our current camera parsing functionality could already capture different berry numbers and types reliably and use the information to evaluate each direction, 2) parsing camera images to identify trees turns out to be quite tricky, and 3) trees themselves do not inherently offer value / danger for the robot.
